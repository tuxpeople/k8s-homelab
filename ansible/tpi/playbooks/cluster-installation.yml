---
- hosts:
    - master
    - worker
  become: true
  gather_facts: true
  any_errors_fatal: true
  pre_tasks:
    - name: Pausing for 5 seconds...
      ansible.builtin.pause:
        seconds: 5
  tasks:
    - name: Check if cluster is installed
      check_mode: false
      ansible.builtin.stat:
        path: /etc/rancher/k3s/config.yaml
      register: k3s_check_installed

    - name: Ignore manifests templates and urls if the cluster is already installed
      when: k3s_check_installed.stat.exists
      ansible.builtin.set_fact:
        k3s_server_manifests_templates: []
        k3s_server_manifests_urls: []

    - name: Install Kubernetes
      ansible.builtin.include_role:
        name: xanmanning.k3s
        public: true
      vars:
        k3s_state: installed

    - name: Get absolute path to this Git repository
      delegate_to: localhost
      become: false
      run_once: true
      check_mode: false
      ansible.builtin.command: git rev-parse --show-toplevel
      register: repository_path

    - name: Copy kubeconfig to the project directory
      when: k3s_primary_control_node
      ansible.builtin.fetch:
        src: /etc/rancher/k3s/k3s.yaml
        dest: "{{ repository_path.stdout }}/kubeconfig-tpi"
        flat: true

    - name: Update kubeconfig with the correct load balancer address
      delegate_to: localhost
      become: false
      run_once: true
      ansible.builtin.replace:
        path: "{{ repository_path.stdout }}/kubeconfig-tpi"
        regexp: https://127.0.0.1:6443
        replace: "https://{{ k3s_registration_address }}:6443"

    - name: Adjust cluster name in Kubeconfig
      delegate_to: localhost
      become: false
      run_once: true
      check_mode: false
      ansible.builtin.command: yq e -i "{{ item }} = \"tpi\"" {{ repository_path.stdout }}/kubeconfig-tpi
      loop:
        - ".clusters[0].name"
        - ".contexts[0].name"
        - ".contexts[0].context.cluster"
        - ".current-context"
# TODO
#   yq e -i ".clusters[0].name = \"homelab\"" kubeconfig
#   yq e -i ".contexts[0].name = \"homelab\"" kubeconfig
#   yq e -i ".contexts[0].context.cluster = \"homelab\"" kubeconfig
#   yq e -i ".current-context = \"homelab\"" kubeconfig

    - name: Copy kubeconfig to target
      run_once: true
      ansible.builtin.copy:
        src: "{{ repository_path.stdout }}/kubeconfig-tpi"
        dest: "{{ kubeconfig_target }}"
      delegate_to: localhost
      become: false

    - name: Post installation of custom manifests
      when:
        - k3s_primary_control_node
        - (k3s_server_manifests_templates | length > 0
            or k3s_server_manifests_urls | length > 0)
      block:
        - name: Wait for custom manifests to rollout
          kubernetes.core.k8s_info:
            kubeconfig: /etc/rancher/k3s/k3s.yaml
            kind: "{{ item.kind }}"
            name: "{{ item.name }}"
            namespace: "{{ item.namespace | default('') }}"
            wait: true
            wait_sleep: 10
            wait_timeout: 360
          loop:
            - name: cilium
              kind: HelmChart
              namespace: kube-system
            - name: podmonitors.monitoring.coreos.com
              kind: CustomResourceDefinition
            - name: prometheusrules.monitoring.coreos.com
              kind: CustomResourceDefinition
            - name: servicemonitors.monitoring.coreos.com
              kind: CustomResourceDefinition
        - name: Wait for Cilium to rollout
          kubernetes.core.k8s_info:
            kubeconfig: /etc/rancher/k3s/k3s.yaml
            kind: Job
            name: helm-install-cilium
            namespace: kube-system
            wait: true
            wait_condition:
              type: Complete
              status: true
            wait_timeout: 360
        # Unmanage and remove the Cilium HelmChart in-order for
        # flux to take over managing the lifecycle of Cilium
        - name: Patch the Cilium HelmChart to unmanage it
          kubernetes.core.k8s_json_patch:
            kubeconfig: /etc/rancher/k3s/k3s.yaml
            name: cilium
            kind: HelmChart
            namespace: kube-system
            patch:
              - op: add
                path: /metadata/annotations/helmcharts.helm.cattle.io~1unmanaged
                value: "true"
        - name: Remove the Cilium HelmChart CR
          kubernetes.core.k8s:
            kubeconfig: /etc/rancher/k3s/k3s.yaml
            name: cilium
            kind: HelmChart
            namespace: kube-system
            state: absent
        - name: Check if Cilium HelmChart was deleted
          kubernetes.core.k8s_info:
            kubeconfig: /etc/rancher/k3s/k3s.yaml
            name: cilium
            kind: HelmChart
            namespace: kube-system
          register: cilium_helmchart
        - name: Force delete the Cilium HelmChart
          when: cilium_helmchart.resources | count > 0
          kubernetes.core.k8s:
            kubeconfig: /etc/rancher/k3s/k3s.yaml
            name: cilium
            kind: HelmChart
            namespace: kube-system
            state: patched
            definition:
              metadata:
                finalizers: []

    # Cleaning up certain manifests from the /var/lib/rancher/k3s/server/manifests directory
    # is needed because k3s has an awesome "feature" to always re-deploy them when the k3s
    # service is restarted. Removing them does not uninstall the manifests from your cluster.
    - name: Delete custom manifests
      when: k3s_primary_control_node
      block:
        - name: Get a list of all custom manifest files
          ansible.builtin.find:
            paths: "{{ k3s_server_manifests_dir }}"
            file_type: file
            use_regex: true
            patterns: ["^custom-.*"]
          register: custom_manifest
        - name: Delete all custom manifest files
          ansible.builtin.file:
            path: "{{ item.path }}"
            state: absent
          loop: "{{ custom_manifest.files }}"
