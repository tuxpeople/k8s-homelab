---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: observability
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 67.11.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: flux-system
  install:
    crds: Skip
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    crds: Skip
    remediation:
      strategy: rollback
      retries: 3
  dependsOn:
    - name: prometheus-operator-crds
      namespace: observability
  values:
    crds:
      enabled: false
    cleanPrometheusOperatorObjectNames: true
    alertmanager:
      enabled: true
      config:
        global:
          slack_api_url: ${SECRET_ALERT_MANAGER_DISCORD_WEBHOOK}
          resolve_timeout: 5m
        #  smtp_smarthost: smtp.gmail.com:587
        #  smtp_auth_username: you@gmail.com
        #  smtp_auth_password: yourapppassword  # https://support.google.com/mail/answer/185833?hl=en-GB
        #  smtp_auth_identity: you@gmail.com
        route:
          group_by:
            - alertname
            - job
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 6h
          receiver: discord
          routes:
            - receiver: "null"
              match:
                alertname: InfoInhibitor
            - receiver: "null"
              match:
                alertname: CPUThrottlingHigh
            - receiver: DeadMansSnitch
              repeat_interval: 30m
              match:
                alertname: Watchdog
            - receiver: discord
              matchers:
                - severity = "critical"
              continue: true
              # - receiver: discord
              #   group_wait: 10s
              #   match_re:
              #     issue: Portworx*
              #   continue: true
        receivers:
          - name: "null"
          - name: email
            email_configs:
              - send_resolved: true
                to: ${SECRET_ACME_EMAIL}
                from: prometheus@tuxpeople.org
                smarthost: smtp.utils.svc.cluster.local:25
                require_tls: false
          - name: DeadMansSnitch
            webhook_configs:
              - url: https://nosnch.in/c15491ac44
                send_resolved: false
          - name: discord
            webhook_configs:
              - send_resolved: true
                url: http://alertmanager-discord:9094
                # title: |-
                #   [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }}{{ else }}{{ .CommonLabels.alertname }}{{ end }}
                # text: >-
                #   {{ range .Alerts -}}
                #     **Alert:** {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}
                #   **Description:** {{ if ne .Annotations.description ""}}{{ .Annotations.description }}{{else}}N/A{{ end }}
                #   **Details:**
                #     {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
                #     {{ end }}
                #   {{ end }}
          # - name: discord
          #   webhook_configs:
          #     - send_resolved: true
          #       url: 'http://alertmanager-discord:9094'
        # Inhibition rules allow to mute a set of alerts given that another alert is firing.
        # We use this to mute any warning-level notifications if the same alert is already critical.
        inhibit_rules:
          - source_matchers:
              - severity = "critical"
            target_matchers:
              - severity = "warning"
            equal:
              - alertname
              - namespace
      alertmanagerSpec:
        replicas: 1
        podAntiAffinity: hard
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: ${MAIN_SC}
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
        resources:
          limits:
            cpu: 500m
            memory: 400Mi
          requests:
            cpu: 5m
            memory: 50Mi
            # priorityClassName: high-priority
      alertmanagerConfigSelector:
        matchLabels:
          alertmanagerConfig: homelab
      ingress:
        enabled: true
        pathType: Prefix
        ingressClassName: internal
        annotations:
          hajimari.io/enable: "true"
          hajimari.io/appName: Alertmanager
          hajimari.io/icon: mdi:alert-decagram-outline
        tls:
          - secretName: ${SECRET_DOMAIN/./-}-production-tls
            hosts:
              - alertmanager.${SECRET_DOMAIN}
        hosts:
          - alertmanager.${SECRET_DOMAIN}
    grafana:
      enabled: false
      forceDeployDashboards: true
      sidecar:
        dashboards:
          multicluster:
            etcd:
              enabled: true
    kubeStateMetrics:
      enabled: true
    kube-state-metrics:
      metricLabelsAllowlist:
        - pods=[*]
        - deployments=[*]
        - persistentvolumeclaims=[*]
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: [__meta_kubernetes_pod_node_name]
              targetLabel: kubernetes_node
    kubelet:
      enabled: true
      serviceMonitor:
        cAdvisor: true
        cAdvisorMetricRelabelings: []
        cAdvisorRelabelings:
          - sourceLabels:
              - __metrics_path__
            targetLabel: metrics_path
        metricRelabelings:
          # Remove duplicate metrics
          - sourceLabels: [__name__]
            regex: (apiserver_audit|apiserver_client|apiserver_delegated|apiserver_envelope|apiserver_storage|apiserver_webhooks|authentication_token|cadvisor_version|container_blkio|container_cpu|container_fs|container_last|container_memory|container_network|container_oom|container_processes|container|csi_operations|disabled_metric|get_token|go|hidden_metric|kubelet_certificate|kubelet_cgroup|kubelet_container|kubelet_containers|kubelet_cpu|kubelet_device|kubelet_graceful|kubelet_http|kubelet_lifecycle|kubelet_managed|kubelet_node|kubelet_pleg|kubelet_pod|kubelet_run|kubelet_running|kubelet_runtime|kubelet_server|kubelet_started|kubelet_volume|kubernetes_build|kubernetes_feature|machine_cpu|machine_memory|machine_nvm|machine_scrape|node_namespace|plugin_manager|prober_probe|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|storage_operation|volume_manager|volume_operation|workqueue)_(.+)
            action: keep
          - sourceLabels: [node]
            targetLabel: instance
            action: replace
    kubeApiServer:
      enabled: true
      serviceMonitor:
        metricRelabelings:
          # Remove duplicate metrics
          - sourceLabels: [__name__]
            regex: (aggregator_openapi|aggregator_unavailable|apiextensions_openapi|apiserver_admission|apiserver_audit|apiserver_cache|apiserver_cel|apiserver_client|apiserver_crd|apiserver_current|apiserver_envelope|apiserver_flowcontrol|apiserver_init|apiserver_kube|apiserver_longrunning|apiserver_request|apiserver_requested|apiserver_response|apiserver_selfrequest|apiserver_storage|apiserver_terminated|apiserver_tls|apiserver_watch|apiserver_webhooks|authenticated_user|authentication|disabled_metric|etcd_bookmark|etcd_lease|etcd_request|field_validation|get_token|go|grpc_client|hidden_metric|kube_apiserver|kubernetes_build|kubernetes_feature|node_authorizer|pod_security|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scrape_duration|scrape_samples|scrape_series|serviceaccount_legacy|serviceaccount_stale|serviceaccount_valid|watch_cache|workqueue)_(.+)
            action: keep
          # Remove high cardinality metrics
          - sourceLabels: [__name__]
            regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
            action: drop
          - sourceLabels: [__name__]
            regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)
            action: drop
    kubeControllerManager:
      enabled: true
      endpoints: &cp
        - 192.168.13.11
        - 192.168.13.12
        - 192.168.13.13
      serviceMonitor:
        metricRelabelings:
          # Remove duplicate metrics
          - sourceLabels: [__name__]
            regex: (apiserver_audit|apiserver_client|apiserver_delegated|apiserver_envelope|apiserver_storage|apiserver_webhooks|attachdetach_controller|authenticated_user|authentication|cronjob_controller|disabled_metric|endpoint_slice|ephemeral_volume|garbagecollector_controller|get_token|go|hidden_metric|job_controller|kubernetes_build|kubernetes_feature|leader_election|node_collector|node_ipam|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|pv_collector|registered_metric|replicaset_controller|rest_client|retroactive_storageclass|root_ca|running_managed|scrape_duration|scrape_samples|scrape_series|service_controller|storage_count|storage_operation|ttl_after|volume_operation|workqueue)_(.+)
            action: keep
    kubeEtcd:
      enabled: true
      endpoints: *cp
    kubeProxy:
      enabled: false # Disabled because eBPF
    kubeScheduler:
      enabled: true
      endpoints: *cp
      serviceMonitor:
        metricRelabelings:
          # Remove duplicate metrics
          - sourceLabels: [__name__]
            regex: (apiserver_audit|apiserver_client|apiserver_delegated|apiserver_envelope|apiserver_storage|apiserver_webhooks|authenticated_user|authentication|disabled_metric|go|hidden_metric|kubernetes_build|kubernetes_feature|leader_election|process_cpu|process_max|process_open|process_resident|process_start|process_virtual|registered_metric|rest_client|scheduler|scrape_duration|scrape_samples|scrape_series|workqueue)_(.+)
            action: keep
    prometheus:
      extraFlags:
        - --web.enable-lifecycle
      ingress:
        enabled: true
        ingressClassName: internal
        annotations:
          hajimari.io/appName: Prometheus
          hajimari.io/icon: simple-icons:prometheus
          nginx.ingress.kubernetes.io/ssl-redirect: "false"
        pathType: Prefix
        hosts:
          - &host prometheus.${SECRET_DOMAIN}
        tls:
          - hosts:
              - *host
      prometheusSpec:
        replicas: 1
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        retentionSize: 2GB
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: ${MAIN_SC}
              resources:
                requests:
                  storage: 12Gi
        scrapeConfigNamespaceSelector:
          matchExpressions:
            - key: kubernetes.io/metadata.name
              operator: Exists
        # additionalScrapeConfigs:
        #   - job_name: node-exporter
        #     scrape_interval: 1m
        #     scrape_timeout: 30s
        #     honor_timestamps: true
        #     # basic_auth:
        #     #   username: randomuser
        #     #   password: examplepassword
        #     static_configs:
        #       - targets: # k3s-node1
        #           - 192.168.13.11:9100
        #         labels:
        #           node: "k3s-node1"
        #       - targets: # k3s-node2
        #           - 192.168.13.12:9100
        #         labels:
        #           node: "k3s-node2"
        #       - targets: # k3s-node3
        #           - 192.168.13.13:9100
        #         labels:
        #           node: "k3s-node3"
        #       - targets: # k3s-node4
        #           - 192.168.13.14:9100
        #         labels:
        #           node: "k3s-node4"
        #       - targets: # NAS
        #           - 10.20.30.40:9100
        #         labels:
        #           node: "nas"
        #       - targets: # laptop2
        #           - laptop2.home:9100
        #         labels:
        #           node: "laptop-work"
    prometheus-node-exporter:
      extraArgs:
        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+|boot/firmware)($|/)
      #  # From archive
    # prometheus-node-exporter:
    #   fullnameOverride: node-exporter
    #   prometheus:
    #     monitor:
    #       enabled: true
    #       relabelings:
    #         - action: replace
    #           regex: (.*)
    #           replacement: $1
    #           sourceLabels:
    #             - __meta_kubernetes_pod_node_name
    #           targetLabel: kubernetes_node
    # prometheusOperator:
    #   prometheusConfigReloader:
    #     resources:
    #       requests:
    #         cpu: 100m
    #         memory: 50Mi
    #       limits:
    #         cpu: 200m
    #         memory: 100Mi
    # prometheus:
    #   enabled: true
    #   persistentVolume:
    #     enabled: true
    #     size: 10Gi
    #   thanosService:
    #     enabled: true
    #   thanosServiceMonitor:
    #     enabled: true
    #   prometheusSpec:
    #     replicas: 1
    #     externalLabels:
    #       cluster: homelab
    #     thanos:
    #       image: quay.io/thanos/thanos:v0.31.0
    #       objectStorageConfig:
    #         name: thanos-objstore-secret
    #         key: objstore.yml
    #     retention: 12h
    #     retentionSize: 10GB
    #     podAntiAffinity: hard
    #     replicaExternalLabelName: __replica__
    #     scrapeInterval: 1m
    #     ruleSelectorNilUsesHelmValues: false
    #     serviceMonitorSelectorNilUsesHelmValues: false
    #     podMonitorSelectorNilUsesHelmValues: false
    #     probeSelectorNilUsesHelmValues: false
    #     enableAdminAPI: true
    #     walCompression: true
    #     disableCompaction: true
    #     storageSpec:
    #       volumeClaimTemplate:
    #         spec:
    #           storageClassName: ${MAIN_SC}
    #           resources:
    #             requests:
    #               storage: 10Gi
    #     resources:
    #       requests:
    #         cpu: 10m
    #         memory: 2000Mi
    #       limits:
    #         memory: 8000Mi
    #     additionalScrapeConfigs:
    #       # - job_name: minio
    #       #   honor_timestamps: true
    #       #   metrics_path: /minio/v2/metrics/cluster
    #       #   static_configs:
    #       #     - targets:
    #       #         - "minio.domain.com:9000"
    #       - job_name: octoprint
    #         scrape_interval: 1m
    #         metrics_path: /plugin/prometheus_exporter/metrics
    #         params:
    #           apikey:
    #             - ${SECRET_OCTOPRINTAPI}
    #         static_configs:
    #           - targets:
    #               - octopi.home:80
    #       - job_name: speedtest-exporter
    #         scrape_interval: 1m
    #         scrape_timeout: 30s
    #         static_configs:
    #           - targets:
    #               - speedtest-exporter:9090
    #       - job_name: minio-job
    #         bearer_token: ${SECRET_MINIO_BEARERTOKEN}
    #         metrics_path: /minio/v2/metrics/cluster
    #         scheme: http
    #         static_configs:
    #           - targets:
    #               - minio.lab.tdeutsch.ch:9091
    #       - job_name: mystrom-exporter
    #         scrape_interval: 1m
    #         metrics_path: /device
    #         honor_labels: true
    #         static_configs:
    #           - targets:
    #               - 10.20.30.33
    #             labels:
    #               alias: 3D Drucker
    #         relabel_configs:
    #           - source_labels:
    #               - __address__
    #             target_label: __param_target
    #           - target_label: __address__
    #             replacement: mystrom-3dprinter:9452
    #       - job_name: prometheus-pushgateway
    #         scrape_interval: 1m
    #         scrape_timeout: 30s
    #         honor_labels: true
    #         static_configs:
    #           - targets:
    #               - prometheus-pushgateway:9091
    #       - job_name: wireguard-exporter
    #         scrape_interval: 1m
    #         scrape_timeout: 30s
    #         metrics_path: /metrics
    #         static_configs:
    #           - targets:
    #               - 10.20.30.1:9586
    #       - job_name: node-exporter
    #         scrape_interval: 1m
    #         scrape_timeout: 30s
    #         honor_timestamps: true
    #         # basic_auth:
    #         #   username: randomuser
    #         #   password: examplepassword
    #         static_configs:
    #           - targets:  # k3s-node01
    #               - 192.168.13.11:9100
    #           - targets:  # k3s-node02
    #               - 192.168.13.12:9100
    #           - targets:  # k3s-node03
    #               - 192.168.13.13:9100
    #           - targets:  # NAS
    #               - 10.20.30.40:9100
    #   ingress:
    #     enabled: true
    #     pathType: Prefix
    #     ingressClassName: internal
    #     annotations:
    #       nginx.ingress.kubernetes.io/auth-method: 'GET'
    # nginx.ingress.kubernetes.io/auth-url: "https://auth.${SECRET_DOMAIN}/api/authz/auth-request"
    # nginx.ingress.kubernetes.io/auth-signin: "https://auth.${SECRET_DOMAIN}?rm=$request_method"
    # nginx.ingress.kubernetes.io/auth-response-headers: "Remote-User,Remote-Name,Remote-Groups,Remote-Email"
    #       hajimari.io/enable: "true"
    #       hajimari.io/appName: Prometheus
    #       hajimari.io/icon: mdi:fire
    #     tls:
    #       - secretName: ${SECRET_DOMAIN/./-}-production-tls
    #         hosts:
    #           - prometheus.${SECRET_DOMAIN}
    #     hosts:
    #       - prometheus.${SECRET_DOMAIN}
